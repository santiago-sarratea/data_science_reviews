{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Math operations\n",
    "import seaborn as sns #Figures and graphics\n",
    "import matplotlib.pyplot as plt #Figure and graphics\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "import pandas as pd #Data analysis\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob#Give us the subjetivity of a text\n",
    "import scipy.stats as stats #statistics tools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pysentimiento import create_analyzer #Give us the sentiment of a text\n",
    "import spacy\n",
    "from statsmodels.stats import multitest\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_utils import preproc, permutation #We use the module that we've created to simulate and pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task two: Is there any difference in the review quality between the two types of users: incentivized one and organic one?\n",
    "Our goal is to define the quality of the reviews and then compare it between incentivized and organic groups. To do this we must find the parameters that define quality and perform statistical tests that allow us to decide if both samples belong to the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have modularized the data pre-processing. This involves deletion of rows in languages other than English to make the analysis more accurate. There are 35 non-English reviews over 2150 total so it is safe to just remove them.\n",
    "Furthermore we fill NaN values, and aggregate new columns to the original dataset. The new columns are: \n",
    "- 'label': contains a 1 if the review is orgnic and a 0 if it is not.\n",
    "- 'all_quest_answ': contains a 1 if all the questions are answered within the review and 0 otherwise.\n",
    "- 'review_text': this column contains the full review text, concatenating all 4 questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "df = pd.read_excel('formatted_review_Asana.xlsx', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Apply the pre-process function to our dataset\n",
    "df = preproc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View of the fisrt 5 rows of df\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "We are going to extract the following features related to text sentiment.\n",
    "- Positivity/negativity: this is a floating point estimator representing the probability that the sentiment of the text is positive/negative. We obtain this metric using the pysentimiento library, which uses BERT algorithm to obtain the sentiment of texts. BERT is an open source ML framework for natural language processing (NLP), having the advantage of being able to use surrounding text to establish context. The BERT framework was pre-trained using text from Wikipedia and social-media.\n",
    "\n",
    "- Subjetivity: this is a measure of the degree of the author's pesonal opinion in the text, represented by a floating point number between 0 (completely objective text) and 1 (completely subjective). We use spacytextblob for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"en\")\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "sp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: this review clearly has a positive sentiment, which should correlate with a high positivity probability, and a certain degree of personal opinion, which should result in a non-negligible subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_review = df.loc[5,'question1']\n",
    "print(ex_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Review sentiment probabilities: {analyzer.predict(ex_review).probas}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this sentence has a very high probability of being classified as Positive (POS), and negligible probabilities of being Neutral (NEU) or Negative (NEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Review subjectivity: {sp(ex_review)._.blob.subjectivity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, a subjectivity of 0.64, indicating the presence of the author's personal opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus find the mentioned indicators for each of the text columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "lista=[[],[],[],[]]\n",
    "\n",
    "for q in range(1,5):\n",
    "    for row in tqdm(df['question'+str(q)]):\n",
    "        lista[q-1].append(analyzer.predict(row).probas)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rev=[]\n",
    "for row in tqdm(df['review_text']):\n",
    "    l_rev.append(analyzer.predict(row).probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lista_sub=[[],[],[],[]]\n",
    "for q in range(1,5):\n",
    "    for row in tqdm(df['question'+str(q)]):\n",
    "        lista_sub[q-1].append(sp(row)._.blob.subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rev_s=[]\n",
    "for row in tqdm(df['review_text']):\n",
    "    l_rev_s.append(sp(row)._.blob.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "for i in range(0, 4):\n",
    "    df_sent_q = pd.DataFrame(lista[i])\n",
    "    df_sent_q.columns = [\n",
    "        \"NEG_q\" + str(i + 1),\n",
    "        \"NEU_q\" + str(i + 1),\n",
    "        \"POS_q\" + str(i + 1),\n",
    "    ]\n",
    "\n",
    "    df_subg_q = pd.DataFrame(lista_sub[i], columns=[\"subj_q\" + str(i + 1)])\n",
    "    data = pd.concat([data, df_sent_q, df_subg_q], axis=1)\n",
    "\n",
    "\n",
    "df_sent_rev = pd.DataFrame(l_rev)\n",
    "df_sent_rev.columns = [\"NEG_rev\", \"NEU_rev\", \"POS_rev\"]\n",
    "\n",
    "df_subj_rev = pd.DataFrame(l_rev_s, columns=[\"subj_rev\"])\n",
    "\n",
    "data = pd.concat([data, df_sent_rev, df_subj_rev], axis=1)\n",
    "data[\"all_quest_answ\"] = df.loc[:, \"all_quest_answ\"]\n",
    "data[\"star\"] = df.loc[:, \"star\"]\n",
    "data[\"label\"] = df.loc[:, \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have the data with the parameters that define the quality of the reviews saved in an external file. This way it is easier to get the final results, although you can also run all the lines of this notebook and get the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"quality_reviews.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we separete the data with two conditions\n",
    "org = data['label']==1 #organic\n",
    "inc = data['label']!=0 #incentivized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of quality: We define the quality of the review based on 4 indicators: positivity, negativity, subjectivity and the fact that all users have answered all the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the distributions of positivity, negativity and subjectivity for the organic and incentivized groups. Finally we also show the distribution of values of the ' ' column. The images have the names rev_pos, rev_neg, rev_sub and questions_answ respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data, col=\"label\", height=5, aspect=1.5)\n",
    "g.map(sns.histplot, \"POS_rev\",  stat=\"probability\", bins=20)\n",
    "g.set_axis_labels(\"Review positivity\", \"POS_rev\")\n",
    "#plt.savefig(\"rev_pos.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data, col=\"label\", height=5, aspect=1.5)\n",
    "g.map(sns.histplot, \"NEG_rev\", stat=\"probability\", bins=15)\n",
    "g.set_axis_labels(\"Review negativity\", \"NEG_rev\")\n",
    "#plt.savefig(\"rev_neg.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data, col=\"label\", height=5, aspect=1.5)\n",
    "g.map(sns.histplot, \"subj_rev\", stat=\"probability\", bins=20)\n",
    "plt.xlabel(\"Review subjetivity\")\n",
    "#plt.savefig(\"rev_subj.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statisical tests\n",
    "\n",
    "For all tests we have used as null hypothesis that both samples belong to the same population of the parameters used to define quality.\n",
    "\n",
    "We conduct a permutations test in order to assert whether there is a statistically significant difference between the means of the previously mentioned features. The statistical tests have been done on the column of the dataset containing the text of all the questions concatenated. \n",
    "\n",
    "In order of appearance the images are the named files: sdist_positivity, sdist_negativity, sdist_subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100000\n",
    "mean_dif = data.loc[org,'POS_rev'].mean() - data.loc[~org,'POS_rev'].mean()\n",
    "\n",
    "dist, p_pos, c_l, c_h = permutation(\n",
    "    mean_dif,\n",
    "    data.loc[org,'POS_rev'], \n",
    "    data.loc[~org,'POS_rev'], \n",
    "    niters=n_iters, \n",
    "    dist=True\n",
    ")\n",
    "\n",
    "sns.histplot(data=dist, stat = 'probability', bins=100)\n",
    "plt.axvline(x = -mean_dif , color = 'red', label='Negative observed difference')\n",
    "plt.axvline(x = mean_dif, color = 'green', label='Observed difference')\n",
    "plt.title(f'Distribution of differences of means after {n_iters} resamples')\n",
    "plt.xlabel('Sample mean difference')\n",
    "plt.legend()\n",
    "#plt.savefig(\"sdist_positivity.pdf\")\n",
    "plt.show()\n",
    "\n",
    "print(f'P-value for review positivity is {p_pos} with a confidence interval ({c_l}, {c_h})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100000\n",
    "mean_dif = data.loc[org,'NEG_rev'].mean() - data.loc[~org,'NEG_rev'].mean()\n",
    "\n",
    "dist, p_neg, c_l, c_h = permutation(\n",
    "    mean_dif, \n",
    "    data.loc[org,'NEG_rev'], \n",
    "    data.loc[~org,'NEG_rev'], \n",
    "    niters=n_iters, \n",
    "    dist=True\n",
    ")\n",
    "\n",
    "sns.histplot(data=dist, stat = 'probability', bins=100)\n",
    "plt.axvline(x = -mean_dif , color = 'red', label='Negative observed difference')\n",
    "plt.axvline(x = mean_dif, color = 'green', label='Observed difference')\n",
    "plt.title(f'Distribution of differences of means after {n_iters} resamples')\n",
    "plt.xlabel('Sample mean difference')\n",
    "plt.legend()\n",
    "#plt.savefig(\"sdist_negativity.pdf\")\n",
    "plt.show()\n",
    "\n",
    "print(f'P-value for review negativity is {p_neg} with a confidence interval ({c_l}, {c_h})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100000\n",
    "mean_dif = data.loc[org,'subj_rev'].mean() - data.loc[~org,'subj_rev'].mean()\n",
    "\n",
    "dist, p_subj, c_l, c_h = permutation(\n",
    "    mean_dif, \n",
    "    data.loc[org,'subj_rev'], \n",
    "    data.loc[~org,'subj_rev'], \n",
    "    niters=n_iters, \n",
    "    dist=True\n",
    ")\n",
    "\n",
    "sns.histplot(data=dist, stat = 'probability', bins=100)\n",
    "plt.axvline(x = -mean_dif , color = 'red', label='Negative observed difference')\n",
    "plt.axvline(x = mean_dif, color = 'green', label='Observed difference')\n",
    "plt.title(f'Distribution of differences of means after {n_iters} resamples')\n",
    "plt.xlabel('Sample mean difference')\n",
    "plt.legend()\n",
    "#plt.savefig(\"sdist_subjectivity.pdf\")\n",
    "plt.show()\n",
    "\n",
    "print(f'P-value for review subjectivity is {p_subj} with a confidence interval ({c_l}, {c_h})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amount of questions answered\n",
    "\n",
    "Another interesting question is whether the user left any of the four questions unanswered. We can see at first glance that there is a difference in this regard among both classes, and we think it could be another useful parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"all_quest_answ_cat\"] = 'Yes'\n",
    "data.loc[data[\"all_quest_answ\"]!=1,\"all_quest_answ_cat\"] = \"No\"\n",
    "\n",
    "g = sns.FacetGrid(data, col=\"label\", height=5, aspect=1.2)\n",
    "g.map(sns.histplot, \"all_quest_answ_cat\", stat=\"probability\", discrete=True)\n",
    "g.set_axis_labels(\"Have all questions been answered?\", \"all_quest_answ_cat\")\n",
    "#plt.savefig(\"questions_answ.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are going to compare with a chi-squared test the ratio of reviews that give answers to all of the four questions between both groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.array(\n",
    "    [[data[org][data[org]['all_quest_answ']==1].shape[0],\n",
    "      data[org][data[org]['all_quest_answ']!=1].shape[0]],\n",
    "     [data[~org][data[~org]['all_quest_answ']==1].shape[0],\n",
    "      data[~org][data[~org]['all_quest_answ']!=1].shape[0]]]\n",
    ")\n",
    "\n",
    "pval_chi = stats.chi2_contingency(T,correction=False)[1]\n",
    "\n",
    "print(f\"P-value for chi-squared test is {pval_chi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final result\n",
    "\n",
    "We finally correct the p-values obtained to account for the amount of tests conducted, using an FDR correction. This is done because when performing multiple comparison tests, the probability of finding a test that tells us that the two populations are different is higher, and this corresponds to a higher probability of obtaining type 1 errors (rejecting the null hypothesis when it is true). The significance level that we used is $\\alpha=0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This object has two components, the first is an array that tell us if the null hypthosis is rejected or not\n",
    "#the second component is an other array, it contains the corrected p-values.\n",
    "multitest.fdrcorrection([p_subj,pval_chi,p_neg, p_pos], alpha=0.05, method='indep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that by performing statistical tests on the difference in means of the parameters that define the quality of the reviews, we have obtained differences between the samples by studying negativity and whether the user answered all the questions or not. On the other hand, the results of the positivity and subjectivity tests indicate that we cannot reject the null hypothesis. For a random seed of permutation tests the results were as follows:\n",
    "\n",
    "\n",
    "$p_{subj}=0.094 >\\alpha=0.05$ It implies not being able to reject the null hypothesis (subjectivity parameter).\n",
    "\n",
    "$p_{val-chi}=0.009 <\\alpha=0.05$ Implies rejecting the null hypothesis (Parameter that counted whether all questions were answered or not).\n",
    "\n",
    "$p_{neg}=0.018 <\\alpha=0.05$ It implies rejecting the null hypothesis (Negativity parameter).\n",
    "\n",
    "\n",
    "$p_{pos}=0.064 >\\alpha=0.05$ It implies not being able to reject the null hypothesis (Positivity parameter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Answer: There is a difference between the quality of reviews written by incentivized and non-incentivized users. This difference can be observed in the review negativity and the amount of answered questions. The parameters of positivity and subjectivity cannot be used to assert differences after statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
